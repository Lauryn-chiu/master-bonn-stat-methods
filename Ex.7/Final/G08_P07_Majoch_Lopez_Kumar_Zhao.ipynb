{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **GROUP 8 - Solutions to P07: Bayesian inference**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Students:**\n",
    "- Marek Majoch, <s13mmajo@uni-bonn.de>, M.Sc Astrophysics\n",
    "- Yanhanle Lauryn Zhao, <s19yzhao@uni-bonn.de>, M.Sc Astrophysics\n",
    "- Diana Victoria Lopez Navarro, <s09dlope@uni-bonn.de>, M.Sc Astrophysics\n",
    "- Rutul Kumar, <s23rkuma@uni-bonn.de>, M.Sc Astrophysics\n",
    "\n",
    "**Deadline:** 27. Nov 2024, 13:00 \n",
    "_______________________________________________________\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import beta, norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Posterior mean of Gaussian random variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./data/P1-07a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./data/P1-07b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Posterior mean of Gaussian random variable with unknown variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us repeat the experiment from problem 1, but this time assuming that we do not know the variance in the measurement a priori. Therefore we would like to estimate both the mean and the variance from the data. We assume a *reference prior* on both mean and variance. This results in a uniform prior on $\\mu$ and a uniform prior on $\\log{\\sigma}$ which leads to the joint prior $$\\pi(\\mu, \\sigma) = \\frac{1}{\\sigma}.$$ \n",
    "(i) Using Bayes' Theorem, determine the posterior of the mean by marginalizing the posterior $p(\\mu, \\sigma \\vert D)$ over $\\sigma$ i.e. $$p(\\mu \\vert D) = \\int p(\\mu, \\sigma \\vert D) d\\sigma.$$\n",
    "(ii) Do you recognize the distribution? What is the difference with our earlier discussion of this distribution?\n",
    "\n",
    "**Note**: A reference prior is a prior with which the contribution of the data to the posterior is maximized. This leads to different priors for location and scale parameters (denoted $\\theta$) of a pdf, which we can understand intuitively:\n",
    "* location parameter (measures the location of the pdf, e.g. mean): if we are ignorant about where to center the pdf, we apply a uniform prior on the real axis, i.e. $\\pi(\\theta) \\propto 1$.\n",
    "* scale parameter (measures the dispersion of the pdf, e.g. variance): if we are ignorant about the dispersion of the pdf, we apply a prior that equally treats each order of magnitude i.e. is uniform in $\\log{\\theta}$; this is equivalent to $\\pi(\\theta) \\propto \\frac{1}{\\theta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: The effect of the prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you are performing a coin toss experiment with a friend: essentially, your friend flips a coin $n$ times and you document the outcomes. Based on the outcome after $n$ tosses, you want to estimate the probability of getting a head (i.e. if the coin is fair or not).\n",
    "\n",
    "Let $\\theta$ be the probability of getting a head with a given coin. Then the probability of obtaining $h$ heads when tossing a coin $n$ times is given by the Binomial distribution as $$p(h|\\theta)=\\theta^h(1-\\theta)^{n-h}.$$\n",
    "\n",
    "(i) Let us assume you have made the 1000 observations given in `coin_tosses_1.txt`, where 1 denotes head and 0 denotes tails. Further assume that you trust your friend and assume a flat pior. Use Bayes' theorem to derive the posterior for $\\theta$. Plot the distribution after 10, 50, 100, 500 and 1000 tosses. What do you observe?\n",
    "\n",
    "(ii) You and your friend now repeat the experiment with another coin and obtain the measurements in `coin_tosses_2.txt`. Based on your experience from (i), you assume a Gaussian prior on $\\theta$ centered at 0.5 with a standard deviation of 0.2. Use Bayes' theorem to derive the posterior for $\\theta$. Plot the distribution after 10, 50, 100, 500 and 1000 tosses. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
